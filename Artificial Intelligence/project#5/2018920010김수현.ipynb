{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/tabular-wine/winequality-white.csv'\n",
    "wine = pd.read_csv(url, error_bad_lines=False, delimiter = ';', dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "wineq = torch.tensor(wine.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wineq[:,:-1]\n",
    "target = wineq[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = data.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[train_indices]\n",
    "target_train = target[train_indices]\n",
    "\n",
    "data_val = data[val_indices]\n",
    "target_val = target[val_indices]\n",
    "\n",
    "datan_train = 0.1 * data_train\n",
    "datan_val = 0.1 * data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([979, 11]),\n",
       " torch.Size([979]),\n",
       " torch.Size([3919, 11]),\n",
       " torch.Size([979, 11]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val.shape, target_val.shape, datan_train.shape, datan_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(\n",
    "    linear_model.parameters(),\n",
    "    lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0541, -0.1397,  0.1324,  0.1919, -0.2868, -0.2292, -0.0616,\n",
       "           0.0114, -0.1283, -0.0231, -0.2933]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0996], requires_grad=True)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,\n",
    "                  t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train) # <1>\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "\n",
    "        t_p_val = model(t_u_val) # <1>\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward() # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "                  f\" Validation loss {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 28.6960, Validation loss 28.4262\n",
      "Epoch 1000, Training loss 3.6136, Validation loss 3.7112\n",
      "Epoch 2000, Training loss 3.3340, Validation loss 3.4227\n",
      "Epoch 3000, Training loss 3.0834, Validation loss 3.1645\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0420,  0.1936, -0.2568,  0.0399,  0.2088,  0.0496,  0.3357,\n",
      "         -0.1348, -0.1320,  0.1783,  0.5646]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0815], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(11, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 3000, \n",
    "    optimizer = optimizer,\n",
    "    model = linear_model,\n",
    "    loss_fn = nn.MSELoss(), # <1>\n",
    "    t_u_train = datan_train,\n",
    "    t_u_val = datan_val, \n",
    "    t_c_train = target_train,\n",
    "    t_c_val = target_val)\n",
    "\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(11, 1024), # <1>\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 1)) # <2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1024, 11]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1, 1024]),\n",
       " torch.Size([1])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.shape for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([1024, 11])\n",
      "0.bias torch.Size([1024])\n",
      "2.weight torch.Size([1, 1024])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (hidden_linear): Linear(in_features=11, out_features=1024, bias=True)\n",
       "  (hidden_activation): Tanh()\n",
       "  (output_linear): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "seq_model = nn.Sequential(OrderedDict([\n",
    "    ('hidden_linear', nn.Linear(11, 1024)),\n",
    "    ('hidden_activation', nn.Tanh()),\n",
    "    ('output_linear', nn.Linear(1024, 1))\n",
    "]))\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([1024, 11])\n",
      "hidden_linear.bias torch.Size([1024])\n",
      "output_linear.weight torch.Size([1, 1024])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0133], requires_grad=True)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.7810, Validation loss 0.8140\n",
      "Epoch 1000, Training loss 0.7805, Validation loss 0.8135\n",
      "Epoch 2000, Training loss 0.7802, Validation loss 0.8130\n",
      "Epoch 3000, Training loss 0.7799, Validation loss 0.8127\n",
      "Epoch 4000, Training loss 0.7797, Validation loss 0.8125\n",
      "Epoch 5000, Training loss 0.7795, Validation loss 0.8122\n",
      "output tensor([[5.9006],\n",
      "        [5.8953],\n",
      "        [5.9405],\n",
      "        [5.8663],\n",
      "        [5.8521],\n",
      "        [5.8640],\n",
      "        [5.9047],\n",
      "        [5.8634],\n",
      "        [5.9190],\n",
      "        [5.9110],\n",
      "        [5.8889],\n",
      "        [5.9026],\n",
      "        [5.8850],\n",
      "        [5.8650],\n",
      "        [5.8649],\n",
      "        [5.8501],\n",
      "        [5.8640],\n",
      "        [5.8709],\n",
      "        [5.8631],\n",
      "        [5.8677],\n",
      "        [5.8485],\n",
      "        [5.8637],\n",
      "        [5.9663],\n",
      "        [5.8668],\n",
      "        [5.8848],\n",
      "        [5.8649],\n",
      "        [5.8441],\n",
      "        [5.8768],\n",
      "        [5.8593],\n",
      "        [5.8646],\n",
      "        [5.8784],\n",
      "        [5.8866],\n",
      "        [5.8345],\n",
      "        [5.8788],\n",
      "        [5.8888],\n",
      "        [5.8626],\n",
      "        [5.8733],\n",
      "        [5.8635],\n",
      "        [5.8705],\n",
      "        [5.8811],\n",
      "        [5.9478],\n",
      "        [5.8789],\n",
      "        [5.8727],\n",
      "        [5.8962],\n",
      "        [5.9037],\n",
      "        [5.8698],\n",
      "        [5.8446],\n",
      "        [5.8840],\n",
      "        [5.8867],\n",
      "        [5.8708],\n",
      "        [5.8881],\n",
      "        [5.8568],\n",
      "        [5.9056],\n",
      "        [5.8814],\n",
      "        [5.8562],\n",
      "        [5.8695],\n",
      "        [5.8751],\n",
      "        [5.8672],\n",
      "        [5.8766],\n",
      "        [5.8572],\n",
      "        [5.8851],\n",
      "        [5.8602],\n",
      "        [5.8769],\n",
      "        [5.8660],\n",
      "        [5.8928],\n",
      "        [5.8852],\n",
      "        [5.8708],\n",
      "        [5.9418],\n",
      "        [5.8582],\n",
      "        [5.8554],\n",
      "        [5.8465],\n",
      "        [5.8699],\n",
      "        [5.9016],\n",
      "        [5.8889],\n",
      "        [5.8811],\n",
      "        [5.8832],\n",
      "        [5.8996],\n",
      "        [5.8771],\n",
      "        [5.9026],\n",
      "        [5.8500],\n",
      "        [5.9133],\n",
      "        [5.8489],\n",
      "        [5.8836],\n",
      "        [5.8819],\n",
      "        [5.9056],\n",
      "        [5.9494],\n",
      "        [5.8599],\n",
      "        [5.8429],\n",
      "        [5.8566],\n",
      "        [5.9032],\n",
      "        [5.8990],\n",
      "        [5.8899],\n",
      "        [5.8945],\n",
      "        [5.8571],\n",
      "        [5.8571],\n",
      "        [5.8825],\n",
      "        [5.9423],\n",
      "        [5.8481],\n",
      "        [5.9013],\n",
      "        [5.8798],\n",
      "        [5.8572],\n",
      "        [5.9141],\n",
      "        [5.9021],\n",
      "        [5.9098],\n",
      "        [5.9826],\n",
      "        [5.8515],\n",
      "        [5.8369],\n",
      "        [5.8608],\n",
      "        [5.8823],\n",
      "        [5.8657],\n",
      "        [5.9201],\n",
      "        [5.8724],\n",
      "        [5.8669],\n",
      "        [5.9126],\n",
      "        [5.8801],\n",
      "        [5.8682],\n",
      "        [5.8743],\n",
      "        [6.0120],\n",
      "        [5.8703],\n",
      "        [5.8896],\n",
      "        [5.8901],\n",
      "        [5.8944],\n",
      "        [5.8784],\n",
      "        [5.9148],\n",
      "        [5.8867],\n",
      "        [5.8536],\n",
      "        [5.8809],\n",
      "        [5.8655],\n",
      "        [5.8815],\n",
      "        [5.8886],\n",
      "        [5.8858],\n",
      "        [5.8616],\n",
      "        [5.9048],\n",
      "        [5.8788],\n",
      "        [5.8881],\n",
      "        [5.9859],\n",
      "        [5.8713],\n",
      "        [5.9111],\n",
      "        [5.8757],\n",
      "        [5.8852],\n",
      "        [5.8969],\n",
      "        [5.8595],\n",
      "        [5.8751],\n",
      "        [5.8879],\n",
      "        [5.8877],\n",
      "        [5.8858],\n",
      "        [5.8889],\n",
      "        [5.8725],\n",
      "        [5.8874],\n",
      "        [5.8778],\n",
      "        [5.8967],\n",
      "        [5.8854],\n",
      "        [5.8828],\n",
      "        [5.8826],\n",
      "        [5.8715],\n",
      "        [5.8802],\n",
      "        [5.8977],\n",
      "        [5.8479],\n",
      "        [5.8635],\n",
      "        [5.8853],\n",
      "        [5.8699],\n",
      "        [5.8727],\n",
      "        [6.0195],\n",
      "        [5.9456],\n",
      "        [5.9034],\n",
      "        [5.8977],\n",
      "        [5.8918],\n",
      "        [5.8671],\n",
      "        [5.9021],\n",
      "        [5.8776],\n",
      "        [5.8744],\n",
      "        [4.7272],\n",
      "        [5.9008],\n",
      "        [5.8260],\n",
      "        [5.8839],\n",
      "        [5.8340],\n",
      "        [5.8807],\n",
      "        [5.8828],\n",
      "        [5.8937],\n",
      "        [5.8805],\n",
      "        [5.8892],\n",
      "        [5.8816],\n",
      "        [5.8603],\n",
      "        [5.8708],\n",
      "        [5.8719],\n",
      "        [5.8523],\n",
      "        [5.8755],\n",
      "        [5.8575],\n",
      "        [5.9122],\n",
      "        [5.8920],\n",
      "        [5.8944],\n",
      "        [5.9102],\n",
      "        [5.9104],\n",
      "        [5.8564],\n",
      "        [5.7919],\n",
      "        [5.8768],\n",
      "        [5.8821],\n",
      "        [5.8749],\n",
      "        [5.8499],\n",
      "        [5.9237],\n",
      "        [5.8970],\n",
      "        [5.9213],\n",
      "        [5.9636],\n",
      "        [5.8632],\n",
      "        [5.9155],\n",
      "        [5.8597],\n",
      "        [5.8558],\n",
      "        [5.8646],\n",
      "        [5.8924],\n",
      "        [5.8729],\n",
      "        [5.9425],\n",
      "        [5.8978],\n",
      "        [5.8692],\n",
      "        [5.8906],\n",
      "        [5.8950],\n",
      "        [5.9359],\n",
      "        [5.8855],\n",
      "        [5.9229],\n",
      "        [5.8615],\n",
      "        [5.8665],\n",
      "        [5.8767],\n",
      "        [5.8619],\n",
      "        [5.8978],\n",
      "        [5.8825],\n",
      "        [5.8885],\n",
      "        [5.8650],\n",
      "        [5.9005],\n",
      "        [5.9213],\n",
      "        [5.8830],\n",
      "        [5.8907],\n",
      "        [5.8968],\n",
      "        [5.8931],\n",
      "        [5.8902],\n",
      "        [5.8826],\n",
      "        [5.9092],\n",
      "        [5.9374],\n",
      "        [5.8748],\n",
      "        [5.8649],\n",
      "        [5.8706],\n",
      "        [5.9593],\n",
      "        [5.8723],\n",
      "        [5.8801],\n",
      "        [5.8658],\n",
      "        [5.8526],\n",
      "        [5.8809],\n",
      "        [5.8694],\n",
      "        [5.8967],\n",
      "        [5.8407],\n",
      "        [5.8832],\n",
      "        [5.8579],\n",
      "        [5.8698],\n",
      "        [5.8857],\n",
      "        [5.8837],\n",
      "        [5.9051],\n",
      "        [5.9132],\n",
      "        [5.8731],\n",
      "        [5.8821],\n",
      "        [5.8684],\n",
      "        [5.8636],\n",
      "        [5.8550],\n",
      "        [5.8908],\n",
      "        [5.8875],\n",
      "        [5.8846],\n",
      "        [5.9327],\n",
      "        [5.8799],\n",
      "        [5.8611],\n",
      "        [5.8779],\n",
      "        [5.8811],\n",
      "        [5.8682],\n",
      "        [5.8972],\n",
      "        [5.8709],\n",
      "        [5.8706],\n",
      "        [5.8876],\n",
      "        [5.8857],\n",
      "        [5.8651],\n",
      "        [5.8598],\n",
      "        [5.9059],\n",
      "        [5.8747],\n",
      "        [5.9150],\n",
      "        [5.8937],\n",
      "        [5.8731],\n",
      "        [5.8949],\n",
      "        [5.9011],\n",
      "        [5.9608],\n",
      "        [5.8539],\n",
      "        [5.9191],\n",
      "        [5.8792],\n",
      "        [6.0281],\n",
      "        [5.8869],\n",
      "        [5.8739],\n",
      "        [5.9529],\n",
      "        [5.8744],\n",
      "        [5.8838],\n",
      "        [5.8853],\n",
      "        [5.9212],\n",
      "        [5.8903],\n",
      "        [5.9328],\n",
      "        [5.8786],\n",
      "        [5.8895],\n",
      "        [5.8691],\n",
      "        [5.8763],\n",
      "        [5.9036],\n",
      "        [5.9194],\n",
      "        [5.9230],\n",
      "        [5.8817],\n",
      "        [5.9095],\n",
      "        [5.8828],\n",
      "        [5.8902],\n",
      "        [5.8793],\n",
      "        [5.8987],\n",
      "        [5.8881],\n",
      "        [5.8554],\n",
      "        [5.8520],\n",
      "        [5.8685],\n",
      "        [5.8819],\n",
      "        [5.8799],\n",
      "        [5.8385],\n",
      "        [5.9061],\n",
      "        [5.9238],\n",
      "        [5.8049],\n",
      "        [5.8854],\n",
      "        [5.8633],\n",
      "        [5.8775],\n",
      "        [5.9127],\n",
      "        [5.8913],\n",
      "        [5.8694],\n",
      "        [5.8760],\n",
      "        [5.8809],\n",
      "        [5.8822],\n",
      "        [5.8451],\n",
      "        [5.8756],\n",
      "        [5.8696],\n",
      "        [5.8597],\n",
      "        [5.8675],\n",
      "        [5.8877],\n",
      "        [5.8776],\n",
      "        [5.8767],\n",
      "        [5.8682],\n",
      "        [5.9280],\n",
      "        [5.8880],\n",
      "        [5.8716],\n",
      "        [5.9029],\n",
      "        [5.8525],\n",
      "        [5.8826],\n",
      "        [5.8656],\n",
      "        [5.8672],\n",
      "        [5.8283],\n",
      "        [5.8510],\n",
      "        [5.8893],\n",
      "        [5.8645],\n",
      "        [5.8810],\n",
      "        [5.8617],\n",
      "        [5.8857],\n",
      "        [5.8926],\n",
      "        [5.8686],\n",
      "        [5.8769],\n",
      "        [5.8756],\n",
      "        [5.8606],\n",
      "        [5.9059],\n",
      "        [5.8720],\n",
      "        [5.9199],\n",
      "        [5.8752],\n",
      "        [5.8446],\n",
      "        [5.8824],\n",
      "        [5.8913],\n",
      "        [5.8802],\n",
      "        [5.9057],\n",
      "        [5.8641],\n",
      "        [5.8990],\n",
      "        [5.8977],\n",
      "        [5.8763],\n",
      "        [5.8687],\n",
      "        [5.8914],\n",
      "        [5.8726],\n",
      "        [5.9176],\n",
      "        [5.8625],\n",
      "        [5.8776],\n",
      "        [5.8893],\n",
      "        [5.8870],\n",
      "        [5.8614],\n",
      "        [5.8830],\n",
      "        [5.8949],\n",
      "        [5.9593],\n",
      "        [5.8639],\n",
      "        [5.8996],\n",
      "        [5.8916],\n",
      "        [6.0089],\n",
      "        [5.8974],\n",
      "        [5.8884],\n",
      "        [5.9007],\n",
      "        [5.8577],\n",
      "        [5.8494],\n",
      "        [5.8892],\n",
      "        [5.8760],\n",
      "        [5.8596],\n",
      "        [5.9886],\n",
      "        [5.8738],\n",
      "        [5.8997],\n",
      "        [5.8490],\n",
      "        [5.8879],\n",
      "        [5.9461],\n",
      "        [5.9063],\n",
      "        [5.9335],\n",
      "        [5.8665],\n",
      "        [5.9025],\n",
      "        [5.8600],\n",
      "        [5.8704],\n",
      "        [5.8819],\n",
      "        [5.8473],\n",
      "        [5.8945],\n",
      "        [5.8456],\n",
      "        [5.8666],\n",
      "        [5.8980],\n",
      "        [5.9155],\n",
      "        [5.8858],\n",
      "        [5.8543],\n",
      "        [5.8848],\n",
      "        [5.8790],\n",
      "        [5.9385],\n",
      "        [5.8625],\n",
      "        [5.9110],\n",
      "        [5.8532],\n",
      "        [5.9095],\n",
      "        [5.8985],\n",
      "        [5.8873],\n",
      "        [5.8849],\n",
      "        [5.8310],\n",
      "        [5.8815],\n",
      "        [5.8838],\n",
      "        [5.8481],\n",
      "        [5.9052],\n",
      "        [5.9038],\n",
      "        [5.8770],\n",
      "        [5.8863],\n",
      "        [5.8654],\n",
      "        [5.8676],\n",
      "        [5.8949],\n",
      "        [5.8878],\n",
      "        [5.8679],\n",
      "        [5.9007],\n",
      "        [5.8609],\n",
      "        [5.8883],\n",
      "        [5.8923],\n",
      "        [5.8884],\n",
      "        [5.9048],\n",
      "        [5.8786],\n",
      "        [5.8764],\n",
      "        [5.8708],\n",
      "        [5.8951],\n",
      "        [5.8781],\n",
      "        [5.8945],\n",
      "        [5.9204],\n",
      "        [5.8788],\n",
      "        [5.8864],\n",
      "        [5.8859],\n",
      "        [5.9040],\n",
      "        [5.9750],\n",
      "        [5.8463],\n",
      "        [5.8912],\n",
      "        [5.8972],\n",
      "        [5.8926],\n",
      "        [5.8532],\n",
      "        [5.8827],\n",
      "        [5.8995],\n",
      "        [5.8543],\n",
      "        [5.8812],\n",
      "        [5.9005],\n",
      "        [5.8632],\n",
      "        [5.8793],\n",
      "        [5.8891],\n",
      "        [5.8937],\n",
      "        [5.8929],\n",
      "        [5.9064],\n",
      "        [5.9251],\n",
      "        [5.8845],\n",
      "        [5.8963],\n",
      "        [5.8743],\n",
      "        [5.8861],\n",
      "        [5.8769],\n",
      "        [5.8805],\n",
      "        [5.8763],\n",
      "        [5.8896],\n",
      "        [5.8697],\n",
      "        [5.8627],\n",
      "        [5.9156],\n",
      "        [5.8758],\n",
      "        [5.8858],\n",
      "        [5.9358],\n",
      "        [5.9051],\n",
      "        [5.8667],\n",
      "        [5.8515],\n",
      "        [5.9109],\n",
      "        [5.9122],\n",
      "        [5.8736],\n",
      "        [5.9319],\n",
      "        [5.8446],\n",
      "        [5.8846],\n",
      "        [5.8806],\n",
      "        [5.8424],\n",
      "        [5.8755],\n",
      "        [5.8790],\n",
      "        [5.9034],\n",
      "        [5.8774],\n",
      "        [5.8302],\n",
      "        [5.9005],\n",
      "        [5.8872],\n",
      "        [5.8665],\n",
      "        [5.9403],\n",
      "        [5.8918],\n",
      "        [5.8887],\n",
      "        [6.0987],\n",
      "        [5.8802],\n",
      "        [5.8782],\n",
      "        [5.8947],\n",
      "        [5.8803],\n",
      "        [5.8945],\n",
      "        [5.8820],\n",
      "        [5.8908],\n",
      "        [5.8731],\n",
      "        [5.8788],\n",
      "        [5.8920],\n",
      "        [5.8558],\n",
      "        [5.8646],\n",
      "        [5.9097],\n",
      "        [5.8774],\n",
      "        [5.8589],\n",
      "        [5.8671],\n",
      "        [5.8708],\n",
      "        [5.8973],\n",
      "        [5.8739],\n",
      "        [5.8830],\n",
      "        [5.8922],\n",
      "        [5.8822],\n",
      "        [5.8658],\n",
      "        [5.8487],\n",
      "        [5.8565],\n",
      "        [5.8728],\n",
      "        [5.8841],\n",
      "        [5.9027],\n",
      "        [5.9157],\n",
      "        [5.8906],\n",
      "        [5.8841],\n",
      "        [5.8340],\n",
      "        [5.9174],\n",
      "        [5.9171],\n",
      "        [5.8875],\n",
      "        [5.8973],\n",
      "        [5.8674],\n",
      "        [5.8667],\n",
      "        [5.9133],\n",
      "        [5.8884],\n",
      "        [5.8986],\n",
      "        [5.9790],\n",
      "        [5.8779],\n",
      "        [5.8700],\n",
      "        [5.8732],\n",
      "        [5.8968],\n",
      "        [5.9075],\n",
      "        [5.8912],\n",
      "        [5.8571],\n",
      "        [5.8706],\n",
      "        [5.8590],\n",
      "        [5.8655],\n",
      "        [5.8939],\n",
      "        [5.8975],\n",
      "        [5.8665],\n",
      "        [5.8786],\n",
      "        [5.8579],\n",
      "        [5.8747],\n",
      "        [5.8945],\n",
      "        [5.8888],\n",
      "        [5.9295],\n",
      "        [5.8787],\n",
      "        [5.8686],\n",
      "        [5.8697],\n",
      "        [5.8988],\n",
      "        [5.8617],\n",
      "        [5.9151],\n",
      "        [5.8748],\n",
      "        [5.8592],\n",
      "        [5.8949],\n",
      "        [5.8655],\n",
      "        [5.8382],\n",
      "        [5.9093],\n",
      "        [5.8832],\n",
      "        [5.8640],\n",
      "        [5.9093],\n",
      "        [5.9056],\n",
      "        [5.8395],\n",
      "        [5.9841],\n",
      "        [5.9020],\n",
      "        [5.8727],\n",
      "        [5.8725],\n",
      "        [5.9216],\n",
      "        [5.8879],\n",
      "        [5.9120],\n",
      "        [5.8892],\n",
      "        [5.8653],\n",
      "        [5.8773],\n",
      "        [5.8937],\n",
      "        [5.8630],\n",
      "        [5.8785],\n",
      "        [5.8786],\n",
      "        [5.8925],\n",
      "        [5.8844],\n",
      "        [5.8492],\n",
      "        [5.9427],\n",
      "        [5.9010],\n",
      "        [5.8907],\n",
      "        [5.8748],\n",
      "        [5.9237],\n",
      "        [5.8342],\n",
      "        [5.9152],\n",
      "        [5.8897],\n",
      "        [5.8846],\n",
      "        [5.9051],\n",
      "        [5.8759],\n",
      "        [5.8703],\n",
      "        [5.8832],\n",
      "        [5.8697],\n",
      "        [5.8849],\n",
      "        [5.8830],\n",
      "        [5.9009],\n",
      "        [5.8637],\n",
      "        [5.9221],\n",
      "        [5.8571],\n",
      "        [5.8812],\n",
      "        [5.8637],\n",
      "        [5.8862],\n",
      "        [5.8882],\n",
      "        [5.8940],\n",
      "        [5.9232],\n",
      "        [5.8899],\n",
      "        [5.8620],\n",
      "        [5.8968],\n",
      "        [5.9327],\n",
      "        [5.8731],\n",
      "        [5.8794],\n",
      "        [5.8753],\n",
      "        [5.8346],\n",
      "        [5.8954],\n",
      "        [5.8785],\n",
      "        [5.9525],\n",
      "        [5.8512],\n",
      "        [5.8905],\n",
      "        [5.8332],\n",
      "        [5.8843],\n",
      "        [5.8572],\n",
      "        [5.8824],\n",
      "        [5.8583],\n",
      "        [5.8688],\n",
      "        [5.8775],\n",
      "        [5.8481],\n",
      "        [5.9070],\n",
      "        [5.8570],\n",
      "        [5.8966],\n",
      "        [5.8618],\n",
      "        [5.9110],\n",
      "        [5.8738],\n",
      "        [5.8594],\n",
      "        [5.8746],\n",
      "        [5.8763],\n",
      "        [5.8667],\n",
      "        [5.8945],\n",
      "        [5.8747],\n",
      "        [5.9110],\n",
      "        [5.8574],\n",
      "        [5.9085],\n",
      "        [5.9546],\n",
      "        [5.8865],\n",
      "        [5.8832],\n",
      "        [5.8739],\n",
      "        [5.8852],\n",
      "        [5.8904],\n",
      "        [5.8672],\n",
      "        [5.8907],\n",
      "        [5.8671],\n",
      "        [5.9039],\n",
      "        [5.9241],\n",
      "        [5.8857],\n",
      "        [5.8969],\n",
      "        [5.8811],\n",
      "        [5.8751],\n",
      "        [5.8748],\n",
      "        [5.8743],\n",
      "        [5.9460],\n",
      "        [5.8650],\n",
      "        [5.8906],\n",
      "        [5.9023],\n",
      "        [5.8602],\n",
      "        [5.8606],\n",
      "        [5.9390],\n",
      "        [5.8610],\n",
      "        [5.8446],\n",
      "        [5.8916],\n",
      "        [5.8857],\n",
      "        [5.9295],\n",
      "        [5.9143],\n",
      "        [5.9095],\n",
      "        [5.8384],\n",
      "        [5.8929],\n",
      "        [5.8780],\n",
      "        [5.9108],\n",
      "        [5.9398],\n",
      "        [5.8988],\n",
      "        [5.8586],\n",
      "        [5.8862],\n",
      "        [5.8763],\n",
      "        [5.8702],\n",
      "        [5.8674],\n",
      "        [5.8968],\n",
      "        [5.8725],\n",
      "        [5.8844],\n",
      "        [5.8977],\n",
      "        [5.8910],\n",
      "        [5.8889],\n",
      "        [5.9013],\n",
      "        [5.8660],\n",
      "        [5.8748],\n",
      "        [5.8671],\n",
      "        [5.8656],\n",
      "        [5.8925],\n",
      "        [5.8776],\n",
      "        [5.8675],\n",
      "        [5.8543],\n",
      "        [5.8490],\n",
      "        [5.8674],\n",
      "        [5.8858],\n",
      "        [5.9120],\n",
      "        [5.8725],\n",
      "        [5.8650],\n",
      "        [5.8719],\n",
      "        [5.8520],\n",
      "        [5.8990],\n",
      "        [5.8566],\n",
      "        [5.8656],\n",
      "        [5.8709],\n",
      "        [5.8763],\n",
      "        [5.8932],\n",
      "        [5.8675],\n",
      "        [5.8859],\n",
      "        [5.8805],\n",
      "        [5.8754],\n",
      "        [5.8586],\n",
      "        [5.8605],\n",
      "        [5.8656],\n",
      "        [5.8853],\n",
      "        [5.9370],\n",
      "        [5.8671],\n",
      "        [5.8849],\n",
      "        [5.8555],\n",
      "        [5.8861],\n",
      "        [5.8971],\n",
      "        [5.8536],\n",
      "        [5.8886],\n",
      "        [5.8685],\n",
      "        [5.8775],\n",
      "        [5.8850],\n",
      "        [5.8704],\n",
      "        [5.8437],\n",
      "        [5.8756],\n",
      "        [5.8769],\n",
      "        [5.8839],\n",
      "        [5.8749],\n",
      "        [5.8614],\n",
      "        [5.9072],\n",
      "        [5.8870],\n",
      "        [5.9328],\n",
      "        [5.8802],\n",
      "        [5.8440],\n",
      "        [5.9055],\n",
      "        [5.8956],\n",
      "        [5.8919],\n",
      "        [5.8669],\n",
      "        [5.8855],\n",
      "        [5.8733],\n",
      "        [5.8749],\n",
      "        [5.9041],\n",
      "        [5.8696],\n",
      "        [5.9084],\n",
      "        [5.8756],\n",
      "        [5.8940],\n",
      "        [5.8785],\n",
      "        [5.9760],\n",
      "        [5.8903],\n",
      "        [5.8787],\n",
      "        [5.9031],\n",
      "        [5.9070],\n",
      "        [5.4453],\n",
      "        [5.8456],\n",
      "        [5.8861],\n",
      "        [5.8438],\n",
      "        [5.8974],\n",
      "        [5.8624],\n",
      "        [5.8802],\n",
      "        [5.8645],\n",
      "        [5.8709],\n",
      "        [5.8950],\n",
      "        [5.8769],\n",
      "        [5.8754],\n",
      "        [5.8718],\n",
      "        [5.9392],\n",
      "        [5.8871],\n",
      "        [5.9464],\n",
      "        [5.8661],\n",
      "        [5.8978],\n",
      "        [5.9323],\n",
      "        [5.9396],\n",
      "        [5.8784],\n",
      "        [5.8597],\n",
      "        [5.8766],\n",
      "        [5.8484],\n",
      "        [5.8783],\n",
      "        [5.9096],\n",
      "        [5.9112],\n",
      "        [5.9230],\n",
      "        [5.8755],\n",
      "        [5.8850],\n",
      "        [5.8604],\n",
      "        [5.8828],\n",
      "        [5.8544],\n",
      "        [5.8687],\n",
      "        [5.9159],\n",
      "        [5.8559],\n",
      "        [5.8724],\n",
      "        [5.8719],\n",
      "        [5.8545],\n",
      "        [5.9363],\n",
      "        [5.8605],\n",
      "        [5.8772],\n",
      "        [5.8930],\n",
      "        [5.8560],\n",
      "        [5.8505],\n",
      "        [5.9212],\n",
      "        [5.8730],\n",
      "        [5.8500],\n",
      "        [5.8734],\n",
      "        [5.8984],\n",
      "        [5.8794],\n",
      "        [5.8831],\n",
      "        [5.8756],\n",
      "        [5.8932],\n",
      "        [5.8593],\n",
      "        [5.8306],\n",
      "        [5.8811],\n",
      "        [5.8900],\n",
      "        [5.8710],\n",
      "        [5.8887],\n",
      "        [5.8937],\n",
      "        [5.9412],\n",
      "        [5.8920],\n",
      "        [5.8915],\n",
      "        [5.8796],\n",
      "        [5.8784],\n",
      "        [5.8988],\n",
      "        [5.8938],\n",
      "        [5.8703],\n",
      "        [5.8820],\n",
      "        [5.8854],\n",
      "        [5.8523],\n",
      "        [5.8707],\n",
      "        [5.9086],\n",
      "        [5.8787],\n",
      "        [5.8877],\n",
      "        [5.9376],\n",
      "        [5.8812],\n",
      "        [5.8507],\n",
      "        [5.8699],\n",
      "        [5.8857],\n",
      "        [5.8752],\n",
      "        [5.8869],\n",
      "        [5.8980],\n",
      "        [5.8499],\n",
      "        [5.8849],\n",
      "        [5.8917],\n",
      "        [5.8677],\n",
      "        [5.9079],\n",
      "        [5.8955],\n",
      "        [5.9057],\n",
      "        [5.8815],\n",
      "        [5.8712],\n",
      "        [5.8896],\n",
      "        [5.9155],\n",
      "        [5.8772],\n",
      "        [5.8711],\n",
      "        [5.8746],\n",
      "        [5.8726],\n",
      "        [5.9113],\n",
      "        [5.8891],\n",
      "        [5.8866],\n",
      "        [5.8879],\n",
      "        [5.8617],\n",
      "        [5.8703],\n",
      "        [5.8822],\n",
      "        [5.8882],\n",
      "        [5.8714],\n",
      "        [5.8806],\n",
      "        [5.8972],\n",
      "        [5.8683],\n",
      "        [5.8632],\n",
      "        [5.9237],\n",
      "        [5.8528],\n",
      "        [5.8505],\n",
      "        [5.8925],\n",
      "        [5.8788],\n",
      "        [5.8920],\n",
      "        [5.8920],\n",
      "        [5.8872],\n",
      "        [5.8671],\n",
      "        [5.9088],\n",
      "        [5.8610],\n",
      "        [5.8877],\n",
      "        [5.8799],\n",
      "        [5.8870],\n",
      "        [5.9299],\n",
      "        [5.8614],\n",
      "        [5.9011],\n",
      "        [5.8775],\n",
      "        [5.9258],\n",
      "        [5.8329],\n",
      "        [5.8779],\n",
      "        [5.8730],\n",
      "        [5.8812],\n",
      "        [5.8516],\n",
      "        [5.8694],\n",
      "        [5.8661],\n",
      "        [5.8526],\n",
      "        [5.8597],\n",
      "        [5.8793],\n",
      "        [5.9149],\n",
      "        [5.8783],\n",
      "        [5.8892],\n",
      "        [5.8599],\n",
      "        [5.8945],\n",
      "        [5.8710],\n",
      "        [5.8524],\n",
      "        [5.8545],\n",
      "        [5.8684],\n",
      "        [5.9492],\n",
      "        [5.8686],\n",
      "        [5.8652],\n",
      "        [5.9013],\n",
      "        [5.8519],\n",
      "        [5.8934],\n",
      "        [5.8790],\n",
      "        [5.8667],\n",
      "        [5.8401],\n",
      "        [5.8799],\n",
      "        [5.8830],\n",
      "        [5.8662],\n",
      "        [5.8929],\n",
      "        [5.8990],\n",
      "        [5.9130],\n",
      "        [5.9045],\n",
      "        [5.9716],\n",
      "        [5.8968],\n",
      "        [5.8463],\n",
      "        [5.8571],\n",
      "        [5.8670],\n",
      "        [5.8543],\n",
      "        [5.9128],\n",
      "        [5.9154],\n",
      "        [5.9174],\n",
      "        [5.8635],\n",
      "        [5.8495],\n",
      "        [5.8634],\n",
      "        [5.8680],\n",
      "        [5.9467],\n",
      "        [5.9020],\n",
      "        [5.9463],\n",
      "        [5.8718],\n",
      "        [5.8929],\n",
      "        [5.8876],\n",
      "        [5.8824],\n",
      "        [5.9200],\n",
      "        [5.8848],\n",
      "        [5.8920],\n",
      "        [5.8698],\n",
      "        [5.8925]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([6., 7., 6., 5., 6., 5., 6., 6., 6., 7., 5., 6., 7., 5., 6., 6., 6.,\n",
      "        6., 5., 5., 6., 5., 3., 5., 5., 5., 5., 6., 6., 5., 5., 5., 6., 6.,\n",
      "        5., 6., 5., 6., 6., 5., 4., 6., 6., 6., 7., 5., 6., 7., 5., 6., 6.,\n",
      "        5., 6., 7., 6., 6., 6., 7., 7., 6., 6., 5., 5., 5., 5., 6., 6., 6.,\n",
      "        6., 6., 6., 5., 5., 6., 5., 5., 6., 5., 6., 7., 6., 6., 6., 5., 7.,\n",
      "        6., 5., 7., 6., 6., 5., 5., 7., 6., 6., 5., 6., 6., 6., 5., 6., 5.,\n",
      "        8., 6., 6., 6., 3., 5., 5., 7., 5., 6., 7., 6., 7., 6., 7., 7., 6.,\n",
      "        8., 5., 5., 6., 8., 7., 5., 6., 6., 5., 5., 7., 6., 5., 6., 6., 6.,\n",
      "        5., 6., 5., 6., 5., 5., 7., 5., 6., 7., 5., 5., 7., 7., 5., 6., 7.,\n",
      "        7., 7., 7., 5., 5., 6., 5., 6., 6., 5., 6., 5., 5., 6., 6., 8., 5.,\n",
      "        8., 5., 5., 4., 5., 6., 6., 5., 6., 6., 5., 6., 7., 6., 6., 6., 5.,\n",
      "        5., 7., 8., 6., 5., 6., 6., 6., 8., 6., 8., 5., 7., 6., 6., 5., 6.,\n",
      "        5., 6., 6., 6., 5., 6., 6., 5., 6., 6., 7., 7., 7., 5., 7., 5., 6.,\n",
      "        7., 5., 6., 5., 5., 5., 6., 7., 6., 6., 5., 6., 7., 5., 7., 5., 4.,\n",
      "        7., 7., 6., 6., 7., 6., 7., 5., 5., 5., 6., 6., 7., 6., 5., 6., 6.,\n",
      "        8., 5., 6., 6., 5., 6., 6., 5., 6., 7., 5., 7., 5., 5., 6., 6., 6.,\n",
      "        7., 6., 6., 5., 5., 6., 6., 7., 8., 5., 6., 7., 6., 6., 5., 6., 7.,\n",
      "        5., 7., 5., 7., 5., 7., 7., 9., 5., 6., 7., 6., 7., 5., 7., 7., 6.,\n",
      "        6., 6., 6., 7., 6., 6., 6., 5., 6., 5., 6., 7., 6., 5., 7., 5., 3.,\n",
      "        5., 7., 5., 6., 6., 7., 6., 6., 7., 7., 7., 6., 5., 6., 5., 6., 8.,\n",
      "        7., 6., 6., 6., 7., 7., 4., 7., 5., 6., 6., 7., 7., 4., 5., 6., 5.,\n",
      "        6., 7., 6., 6., 8., 5., 5., 5., 7., 5., 5., 9., 6., 5., 6., 7., 6.,\n",
      "        7., 5., 6., 6., 5., 5., 4., 6., 6., 6., 6., 5., 6., 6., 5., 5., 6.,\n",
      "        6., 7., 6., 6., 7., 5., 6., 6., 6., 5., 5., 5., 7., 5., 5., 6., 6.,\n",
      "        6., 6., 6., 6., 7., 6., 7., 7., 6., 6., 6., 6., 7., 8., 5., 4., 6.,\n",
      "        6., 6., 5., 5., 6., 6., 5., 6., 6., 6., 7., 6., 6., 5., 5., 5., 6.,\n",
      "        5., 5., 5., 4., 6., 8., 8., 6., 7., 5., 6., 5., 6., 6., 6., 6., 7.,\n",
      "        5., 4., 8., 7., 6., 6., 7., 5., 5., 6., 6., 6., 5., 7., 6., 7., 6.,\n",
      "        6., 5., 5., 6., 6., 5., 7., 7., 5., 6., 7., 7., 5., 5., 6., 5., 7.,\n",
      "        7., 5., 5., 6., 6., 5., 5., 5., 6., 5., 5., 5., 4., 5., 5., 7., 7.,\n",
      "        6., 7., 6., 5., 7., 7., 6., 5., 7., 5., 5., 6., 6., 5., 6., 5., 5.,\n",
      "        6., 8., 6., 6., 4., 7., 6., 6., 6., 6., 6., 5., 6., 5., 7., 6., 7.,\n",
      "        7., 7., 4., 5., 6., 5., 5., 6., 7., 6., 5., 6., 6., 6., 6., 5., 7.,\n",
      "        6., 6., 5., 7., 6., 6., 6., 5., 6., 5., 5., 6., 7., 5., 6., 7., 8.,\n",
      "        5., 7., 6., 5., 5., 6., 7., 6., 5., 6., 6., 7., 6., 5., 6., 4., 7.,\n",
      "        7., 6., 5., 7., 6., 6., 6., 8., 5., 6., 5., 6., 5., 6., 5., 7., 5.,\n",
      "        7., 5., 6., 5., 5., 5., 6., 6., 6., 5., 6., 5., 7., 6., 6., 7., 6.,\n",
      "        6., 5., 5., 6., 7., 5., 7., 8., 6., 5., 5., 7., 5., 6., 5., 6., 5.,\n",
      "        5., 6., 4., 6., 5., 5., 7., 6., 6., 6., 5., 6., 6., 7., 6., 5., 6.,\n",
      "        7., 6., 4., 5., 7., 5., 6., 5., 6., 5., 5., 5., 6., 5., 5., 6., 7.,\n",
      "        4., 5., 6., 6., 6., 4., 6., 7., 6., 5., 6., 7., 5., 5., 6., 4., 5.,\n",
      "        7., 5., 5., 6., 8., 5., 6., 8., 6., 6., 5., 5., 4., 6., 7., 5., 6.,\n",
      "        7., 7., 6., 6., 5., 8., 7., 6., 5., 7., 7., 6., 6., 8., 6., 4., 5.,\n",
      "        7., 6., 5., 6., 6., 5., 5., 7., 7., 6., 5., 5., 5., 6., 5., 7., 6.,\n",
      "        5., 4., 8., 7., 5., 5., 6., 7., 5., 7., 6., 6., 6., 6., 6., 6., 5.,\n",
      "        5., 6., 6., 7., 5., 6., 5., 5., 6., 5., 5., 6., 5., 6., 5., 5., 7.,\n",
      "        5., 4., 7., 6., 7., 5., 3., 6., 6., 6., 7., 5., 7., 6., 6., 5., 6.,\n",
      "        6., 6., 8., 5., 6., 6., 5., 5., 6., 7., 6., 7., 6., 7., 6., 6., 7.,\n",
      "        4., 5., 7., 4., 5., 6., 7., 5., 8., 5., 6., 6., 6., 6., 5., 6., 6.,\n",
      "        7., 6., 6., 5., 4., 6., 7., 7., 6., 7., 5., 7., 5., 7., 6., 6., 7.,\n",
      "        8., 7., 6., 5., 6., 5., 7., 5., 5., 4., 6., 6., 5., 6., 6., 7., 6.,\n",
      "        7., 7., 8., 7., 5., 7., 6., 6., 5., 6., 5., 6., 6., 5., 6., 6., 6.,\n",
      "        5., 5., 6., 5., 5., 4., 5., 5., 6., 7., 5., 5., 6., 6., 6., 6., 7.,\n",
      "        6., 6., 6., 6., 8., 6., 5., 7., 5., 6., 6., 5., 5., 6., 4., 7., 6.,\n",
      "        6., 5., 5., 5., 7., 5., 6., 3., 5., 7., 6., 6., 6., 5., 5., 5., 7.,\n",
      "        5., 5., 5., 7., 6., 6., 6., 4., 6., 7., 5., 5., 6., 5., 6., 5., 5.,\n",
      "        5., 7., 4., 6., 5., 6., 5., 7., 6., 6., 7., 5., 6., 5., 6., 6., 5.,\n",
      "        7., 5., 5., 6., 5., 8., 5., 5., 6., 4.])\n",
      "hidden tensor([[ 3.3213e-06,  3.5224e-07,  ...,  3.6120e-07,  3.4523e-06],\n",
      "        [-1.3710e-05, -1.5181e-06,  ..., -1.8213e-06, -1.5848e-05],\n",
      "        ...,\n",
      "        [ 5.9870e-06,  5.2929e-07,  ...,  3.3376e-07,  8.6699e-06],\n",
      "        [ 1.0690e-05,  8.0148e-07,  ...,  5.8163e-07,  1.4660e-05]])\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3) \n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    model = seq_model,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    t_u_train = datan_train,\n",
    "    t_u_val = datan_val, \n",
    "    t_c_train = target_train,\n",
    "    t_c_val = target_val)\n",
    "\n",
    "print('output', seq_model(datan_val))\n",
    "print('answer', target_val)\n",
    "print('hidden', seq_model.hidden_linear.weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
